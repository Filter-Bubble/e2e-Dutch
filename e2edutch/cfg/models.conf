# Word embeddings.
dutch_combined_320 {
  path = combined-320.txt
  size = 320
}
dutch_combined_320_filtered {
  path = combined-320.txt.filtered
  size = 320
}

dutch_uncased_320 {
  path = combined-320.txt
  size = 320
  cased = false
}
dutch_uncased_320_filtered {
  path = combined-320.txt.filtered
  size = 320
  cased = false
}

dutch_fasttext_100 {
  path = fasttext.100.model.vec
  size = 100
  cased = false
}
dutch_fasttext_100_filtered {
  path = fasttext.100.model.vec.filtered
  size = 100
  cased = false
}

dutch_fasttext_300 {
  path = fasttext.300.vec
  size = 300
  cased = true
}
dutch_fasttext_300_filtered {
  path = fasttext.300.vec.filtered
  size = 300
  cased = true
}

# Distributed training configurations.
two_local_gpus {
  addresses {
    ps = [localhost:2222]
    worker = [localhost:2223, localhost:2224]
  }
  gpus = [0, 1]
}

# Main configuration.
best {
  # Computation limits.
  max_top_antecedents = 30
  max_training_sentences = 30
  top_span_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.dutch.txt"
  context_embeddings = ${dutch_fasttext_300_filtered}
  head_embeddings = ${dutch_fasttext_300_filtered}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = false
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 1
  lm_size = 768
  coarse_to_fine = true

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  # lm_path = False
  lm_model_name = "bertje"
  lm_path = bertje_cache.hdf5
  genres = ["all"]
  eval_frequency = 5000
  report_frequency = 100
  use_gold = false

  log_root = logs
  cluster = ${two_local_gpus}
}

test = ${best} 

bertje_fasttext = ${best} {
  eval_path = dev-short.dutch.jsonlines
  conll_eval_path = dev-short.dutch.conll
}


bertje_w2v = ${best} {
  context_embeddings = ${dutch_uncased_320_filtered}
  head_embeddings = ${dutch_uncased_320_filtered}

  eval_path = dev-short.dutch.jsonlines
  conll_eval_path = dev-short.dutch.conll
}

bertje_w2v_eval = ${bertje_w2v} {
  context_embeddings = ${dutch_uncased_320}
  head_embeddings = ${dutch_uncased_320}

  eval_path = dev.dutch.jsonlines
  conll_eval_path = dev.dutch.conll
}

robbert_fasttext = ${best} {
  lm_model_name = "robbert"
  lm_path = robbert_cache.hdf5

  eval_path = dev-short.dutch.jsonlines
  conll_eval_path = dev-short.dutch.conll
}



best_goldmentions = ${best} {
  eval_path = dev.mentions.jsonlines
  conll_eval_path = dev.mentions.gold.conll
  use_gold = True
  lm_path = ""
  context_embeddings = ${dutch_fasttext_300}
  head_embeddings = ${dutch_fasttext_300}
}

bertnl_fasttext = ${best} {
  lm_model_name = "bert-nl"
  lm_path = bert-nl_cache.hdf5

  eval_path = dev-short.dutch.jsonlines
  conll_eval_path = dev-short.dutch.conll
}


bertje_fasttext100 = ${best} {

  context_embeddings = ${dutch_fasttext_100_filtered}
  head_embeddings = ${dutch_fasttext_100_filtered}

  eval_path = dev-short.dutch.jsonlines
  conll_eval_path = dev-short.dutch.conll
}


# For evaluation. Do not use for training (i.e. only for predict.py, evaluate.py, and demo.py). Rename `best` directory to `final`.
final = ${best} {
  context_embeddings = ${dutch_fasttext_300}
  head_embeddings = ${dutch_fasttext_300}
  lm_path = ""
  eval_path = test.dutch.jsonlines
  conll_eval_path = test.dutch.conll
}

gold_mentions = ${final} {
  use_gold = True
}

# Baselines.
c2f_100_ant = ${best} {
  max_top_antecedents = 100
}
c2f_250_ant = ${best} {
  max_top_antecedents = 250
}
c2f_1_layer = ${best} {
  coref_depth = 1
}
c2f_3_layer = ${best} {
  coref_depth = 3
}
distance_50_ant = ${best} {
  max_top_antecedents = 50
  coarse_to_fine = false
  coref_depth = 1
}
distance_100_ant = ${distance_50_ant} {
  max_top_antecedents = 100
}
distance_250_ant = ${distance_50_ant} {
  max_top_antecedents = 250
}
